\section{Методология}
Предложенная модель генеративной аугментации включает несколько компонентов: модель детекции, модель описания нового объекта, модель генерации нового объекта и модель фильтрации.

\subsection{Модель для детекции исходного объекта}
Наша архитектура реализована в полностью автоматическом режиме. Для первичной детекции объекта применяется предварительно обученная модель YOLO, способная обнаруживать объекты и строить для каждого найденного объекта ограничивающий прямоугольник с указанием класса. Среди всех возвращённых ограничивающих прямоугольников система выбирает один с максимальной площадью, что гарантирует фокусировку на наиболее значимом объекте сцены для последующей аугментации. 

Результатом работы детекции являются координаты ограничивающего прямоугольника выбранного для аугментации объекта и его класс, необходимые для передачи в последующие модули архитектуры.

Если пользователь желает аугментировать конкретный объект, он может передать системе маску в формате одноканального изображения, указывающую область для аугментации. В этом случае YOLO не выполняет автоматический выбор и использует переданный ограничивающий прямоугольник. Также пользователю потребуется передать еще класс объекта, которому соответствует переданная маска.

\subsection{Модель для генерации текстового запроса}
Процесс генерации текстового запроса разбит на несколько последовательных этапов, обеспечивающих корректное семантическое соответствие исходного и нового объектов, а также учёт визуального контекста изображения.

\subsubsection{Определение нового класса объекта}
Сначала архитектура получает метку исходного класса объекта из выходных данных детектора YOLO. Затем применяется zero‐shot классификатор~\cite{DBLP:journals/corr/abs-1909-00161} для выбора наиболее вероятного релевантного обобщающего класса \texttt{class\_type}  (например, «air vehicle», «land animal» и т.д.), используя текстовый запрос, содержащий только метку исходного объекта \texttt{base\_class}.

Далее задача заключается в выборе нового класса из списка кандидатов. Для этого используется тот же zero‐shot классификатор с запросом в формате
\texttt{''What \{class\_type\} is similar to a \{base\_class\}?''}
что позволяет модели выбирать правдоподобные замены из заданного списка. Чтобы получить независимые вероятности для каждой метки, включается режим multilabel, при котором вероятность каждой метки рассчитывается отдельно, без нормировки по сумме вероятностей всех кандидатов. В результате формируется вектор оценок—вероятностей для каждого кандидата на замену, после чего отбираются все метки с оценкой выше 0.4 в список. Если после этого он оказывается пустым, то выбирается метка с максимальной вероятностью, что гарантирует наличие замены. Из оставшихся кандидатов система случайным образом выбирает один новый класс, обеспечивая вариативность и использование семантически близких меток. Такой подход позволяет учитывать произвольное число кандидатов и захватывать смысловые пересечения с исходным объектом.

Стоит отметить, что пользователь может самостоятельно задавать список кандидатов для выбора.

\subsubsection{Генерация описания сцены}
Для того чтобы модель аугментации учитывала более широкий визуальный контекст и семантические детали, данный этап включает генерацию текстового описания всего изображения с учётом исходного класса объекта. В качестве инструмента используется BLIP\cite{DBLP:journals/corr/abs-2201-12086}, обученный на наборе пар «изображение–текст» и способный генерировать описания, отражающие содержимое сцены и особенности объектов. При подаче исходного класса BLIP формирует текстовый запрос, подробно описывающий сцену, что позволяет модели получить представление о визуальных признаках и взаимосвязях между объектами.

Стоит отметить, что при передаче сформированного описания в следующую компоненту архитектуры название исходного объекта намеренно скрывается — этот приём используется для того, чтобы модель сосредоточилась на генерации нового объекта и не опиралась на информацию о предыдущем.

\subsubsection{Формирование расширенного запроса}
После выбора нового класса \texttt{new\_object} и формирования описания изображения \texttt{image\_description} модель Qwen3-8B\cite{yang2025qwen3technicalreport} генерирует расширенный запрос для дальнейшей аугментации. Конкретный шаблон запроса выглядит следующим образом:
\begin{verbatim}
USER: Write a concise, realistic visual description of a {new_object} 
in less than 20 words in that scene: {image_description}. 
Don't include background or other objects. Use descriptive terms only 
about the {new_object}. 
Then append style comments like: "4k, ultra HD, highly detailed, 
realistic lighting". 
ASSISTANT:
\end{verbatim}При передаче такого запроса Qwen3-8B выдаёт развёрнутую визуальную подсказку, содержащую лаконичное описание нового объекта в контексте сцены и рекомендации по стилю (качество изображения, детализация, освещение и т.\,п.), что обеспечивает модели генерации объектов необходимую семантическую и визуальную информацию для генерации реалистичных изображений.


\subsection{Модель генерации нового объекта}

Основным компонентом архитектуры является модель генерации нового объекта — FLUX\cite{flux2024}. Эта модель относится к семейству диффузионных и обучается с помощью функции потерь, основанной на Flow Matching\cite{lipman2023flowmatchinggenerativemodeling}.

Рассмотрим неизвестное распредление данных \(q(x)\) с доступным набором выборок из этого распределения. Введём непрерывный путь плотностей \(\{p_t\}_{t\in[0,1]}\), где  
$p_0(x) \;=\; \mathcal{N}(x\mid 0, I)$, а \(p_1(x)\approx q(x)\).

Flow Matching нацелено на то, чтобы обучить параметрическое векторное поле \(v_\omega(x,t)\) так, чтобы оно совпадало с истинным векторным полем \(u_t(x)\), которое задаёт эволюцию плотностей \(p_t\).
Тогда функция потерь Flow Matching записывается следующим образом:
\[
\mathcal{L}_{\mathrm{FM}}(\omega)
\;=\;
\mathbb{E}_{\substack{t\sim \mathcal{U}(0,1) \\ x\sim p_t}}
\bigl\|\,v_\omega(x,t)\;-\;u_t(x)\bigr\|^2_2
\]

При обучении решается следующая оптимизационная задача:

\[
\omega^* = \arg\min_{\omega}\;\mathcal{L}_{FM}(\omega)
\]


На практике проблема в том, что \(p_t\) и \(u_t\) обычно неизвестны в явном виде. Для каждого примера из датасета \(x_0\!\sim p_0\), \(x_1\!\sim p_1\) можно строить локальный путь и локальное поле, а затем агрегировать их , чтобы получить приближённые \(p_t\) и \(u_t\). Это даёт более удобный и вычислительно эффективный подход к Flow Matching. Подробнее можно ознакомиться в статье\cite{lipman2023flowmatchinggenerativemodeling}.


\subsection{Модель фильтрации сгенерированного изображения}
После генерации аугментированных изображений применяется модель фильтрации AlphaCLIP\cite{sun2023alphaclipclipmodelfocusing}, разработанная как модификация архитектуры CLIP\cite{DBLP:journals/corr/abs-2103-00020}. В отличие от базовой модели AlphaCLIP позволяет вычислять семантическое соответствие не только между полным изображением и текстом, но и между локальными областями изображения, заданными бинарной маской, и текстовым описанием объекта.

Для оценки качества сгенерированных аугментаций вычисляется скалярное произведение между латентным представлением текстового описания целевого объекта и латентным представлением области изображения, выделенной с помощью сегментационной маски.

Порог семантического сходства задается пользователем: его регулировка позволяет контролировать строгость фильтрации. Уменьшение порога понижает качество аугментаций.

